<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.62.2" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark">
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? "https://" : "http://");document.write(unescape("%3Cspan style='display:none;' id='cnzz_stat_icon_1278593076'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s4.cnzz.com/z_stat.php%3Fid%3D1278593076%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>


<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11"></script>
<script async>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script><title>Scrapy学习笔记（二）&nbsp;&ndash;&nbsp;简讯</title><link rel="stylesheet" href="/css/core.min.ce9eeb504e74ea6d08f30161645cef372e2adfd6c03640a74eaba10136862b9b178739d9b5916e59dfa3d3b78e45d87a.css" integrity="sha384-zp7rUE506m0I8wFhZFzvNy4q39bANkCnTquhATaGK5sXhznZtZFuWd&#43;j07eORdh6">
<link rel="stylesheet" href="/css/all.css" integrity=""><body>
    <div class="base-body"><section id="header" class="site header max-body-width">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><span class="site name" style="font-size: 24px;">简讯</span></a></span>
        <span class="header right-side"><div class="nav wrap">
    <nav class="nav"><a class="nav item" href="/categories/">
            <i class="fas fa-align-left"></i>&nbsp;分类
        </a><a class="nav item" href="/tags/">
            <i class="fas fa-tags"></i>&nbsp;标签
        </a><a class="nav item" href="/projects">
            
            
            <i class="fas fa-cubes"></i>&nbsp;个人项目</a><a class="nav item" href="/about">
            
            
            <i class="fas fa-feather-alt"></i>&nbsp;关于</a></nav>
</div></span></div>
        <div class="site slogan"><span class="title"></span></div></section>

<script>
    
    var typed = new Typed('.title', {
      strings: ["唯书与爱，不可辜负", "岁月无婷无华"],
      smartBackspace: true,
      typeSpeed: 95,
      backSpeed: 35
    });
</script><div id="content" class="max-body-width"><section class="article header">
    <h1 class="article title">Scrapy学习笔记（二）</h1><p class="article date">Thursday, June 22, 2017<span class="reading-time"> • 4 minutes to read</span></p></section><article class="article markdown-body"><p>通过这篇文章，我们会了解scrapy的命令行工具。</p>
<p>文章介绍命令行工具的顺序基本就是按照官方文档的顺序来。</p>
<h1 id="命令">命令</h1>
<p>scrapy的命令分为全局命令和项目内命令。全局命令也就是不需要在项目所在目录下运行，项目内命令必须在生成项目后，在此目录下运行的命令。举个栗子，<code>startproject</code>就是全局命令，因为在运行这个命令的时候还没有项目，<code>check</code>就是项目内命令，因为必须有项目才能检查代码正确与否。</p>
<h2 id="global-commands">Global commands:</h2>
<ul>
<li>startproject</li>
<li>genspider</li>
<li>settings</li>
<li>runspider</li>
<li>shell</li>
<li>fetch</li>
<li>view</li>
<li>version</li>
</ul>
<h2 id="project-only-commands">Project-only commands:</h2>
<ul>
<li>crawl</li>
<li>check</li>
<li>list</li>
<li>edit</li>
<li>parse</li>
<li>bench</li>
</ul>
<h1 id="创建项目">创建项目</h1>
<p>命令：</p>
<pre><code>scrapy startproject testproject
</code></pre><p>这个命令用于生成我们所需要的爬虫项目。进入到该目录中，会发现生成了许多文件。这些文件的用法在以后都会一一详解。</p>
<p><a href="http://olzlqlgy5.bkt.clouddn.com/Screenshot%20from%202017-06-22%2021-01-41.png"target="_blank"><a target="_blank" rel="noopener noreferrer" 
  href="http://olzlqlgy5.bkt.clouddn.com/Screenshot%20from%202017-06-22%2021-01-41.png"><img  src="http://olzlqlgy5.bkt.clouddn.com/Screenshot%20from%202017-06-22%2021-01-41.png"
        alt="img"/></a></a></p>
<h1 id="生成spider">生成spider</h1>
<p>命令：</p>
<pre><code>scrapy genspider baidu www.baidu.com
</code></pre><p>输入该命令会在spiders文件夹下生成一个名为 baidu.py 的文件，cat这个文件，我们会发现其实就是最基本的spider模板。</p>
<p><a href="http://olzlqlgy5.bkt.clouddn.com/Screenshot%20from%202017-06-22%2023-19-59.png"target="_blank"><a target="_blank" rel="noopener noreferrer" 
  href="http://olzlqlgy5.bkt.clouddn.com/Screenshot%20from%202017-06-22%2023-19-59.png"><img  src="http://olzlqlgy5.bkt.clouddn.com/Screenshot%20from%202017-06-22%2023-19-59.png"
        alt="img"/></a></a></p>
<h2 id="模板选择">模板选择</h2>
<p>在终端输入命令可以查看生成模板的类型：</p>
<pre><code>scrapy genspider -l
</code></pre><p>输出：</p>
<pre><code>Available templates:
  basic
  crawl
  csvfeed
  xmlfeed
</code></pre><p>也就是系统自带这四种模板，如果没有指定模板，会选择<code>basic</code>基本模板。如果你问我可以自定义模板吗，当然可以了，不过本文暂时不涉及这个问题，如果有需要可以先自行google。</p>
<p>使用模板命令：</p>
<pre><code>scrapy genspider -t xmlfeed zhihu www.zhihu.com
</code></pre><p><code>-t</code>就是TEMPLATE，也就是模板。</p>
<p>例子：</p>
<pre><code>$ scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

$ scrapy genspider example example.com
Created spider 'example' using template 'basic'

$ scrapy genspider -t crawl scrapyorg scrapy.org
Created spider 'scrapyorg' using template 'crawl'
</code></pre><h1 id="crawl">crawl</h1>
<p>用于运行指定spider</p>
<p>命令：</p>
<pre><code>scrapy crawl baidu
</code></pre><h1 id="check">check</h1>
<p><code>check</code>用来检查项目中的代码是否有错误。如果没错会返回ok，如果有错会定位错误代码的位置。</p>
<p>命令：</p>
<pre><code>scrapy check
</code></pre><p>示例：</p>
<pre><code>➜  testproject scrapy check

----------------------------------------------------------------------
Ran 0 contracts in 0.000s

OK
</code></pre><h1 id="list">list</h1>
<p>运行命令会列出项目中所有的spider。</p>
<p>命令：</p>
<pre><code>scrapy list
</code></pre><p>示例，在我们创建的项目目录下运行：</p>
<pre><code>➜  testproject scrapy list
baidu
zhihu
</code></pre><h1 id="shell">shell</h1>
<p>运行这个命令会进入命令行交互模式，以给定的 URL(如果给出)或者空(没有给出 URL)启动 Scrapy shell。查看 Scrapy 终端(Scrapy shell)可以做一些简单的操作，可以使用选择器快速获取信息，方便调试。</p>
<h1 id="fetch">fetch</h1>
<p>使用这个命令会执行一次请求，并调用scrapy的下载器，返回网页的源码。</p>
<p>命令：</p>
<pre><code>scrapy fetch http://www.baidu.com
</code></pre><p>还可以加三个参数：</p>
<p><code>--nolog</code>
<code>--headers</code>
<code>--no-redirect</code></p>
<p>分别是不输出日志信息，返回网页的请求头和禁止重定向。如果网页没有重定向的话返回的还是原网页。</p>
<h1 id="view">view</h1>
<p>命令：</p>
<pre><code>scrapy view http://www.taobao.com
</code></pre><p>这个命令比较有用，它的作用是请求网址，输出网址的源码，并将该网页保存成一个文件，使用浏览器打开。如果打开的网址和你正常加载的网页有所不同，一般情况下没显示的部分使用了异步加载。因此该命令可以用来检查 spider 所获取到的页面,并确认这是您所期望的。</p>
<p>这样在你以后的抓取过程中就可以使用这个命令分析网页是否使用了异步加载。</p>
<h1 id="runspider">runspider</h1>
<p>这个命令和crawl命令的区别在于crawl命令后是spider的<code>name</code>，而runspider命令后加的是爬虫的文件名，在本文的项目中，使用crawl命令：</p>
<pre><code>scrapy crawl baidu
</code></pre><p>使用runspider就是：</p>
<pre><code>scrapy runspider baidu.py
</code></pre><h1 id="settings">settings</h1>
<p>用来获取项目的配置信息。</p>
<p>例如获取项目名称：</p>
<pre><code>➜  testproject scrapy settings --get BOT_NAME
testproject
</code></pre><h1 id="edit">edit</h1>
<p>如果你不使用vim作为编辑器的话，这个命令不常用，因为这个命令会调用vim来编辑文件。</p>
<p>命令：</p>
<pre><code>scrapy edit baidu
</code></pre><p>如果你想学习这个编辑器的话，这有三篇入门文章可以供你参考：</p>
<p>1.<a href="http://alpha87.cn/2017/06/22/scrapy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"target="_blank">vim快捷用法</a>
2.<a href="http://alpha87.cn/2017/06/22/scrapy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"target="_blank">对vim的简单配置</a>
3.<a href="http://alpha87.cn/2017/06/22/scrapy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"target="_blank">安装YouCompleteMe插件-Python版</a></p>
<h1 id="parse">parse</h1>
<p>获取给定的 URL 并使用相应的 spider 分析处理。如果您提供 –callback 选项,则使用 spider 的该方法处理,否则使用 parse</p>
<p>使用上一篇的例子：</p>
<pre><code>scrapy parse http://quotes.toscrape.com -c parse
</code></pre><p>支持的操作：</p>
<p>&ndash;spider = SPIDER:
bypass spider autodetection and force use of specific spider
跳过自动检测 spider 并强制使用特定的 spider</p>
<p>&ndash;a NAME = VALUE:
set spider argument (may be repeated)
设置 spider 的参数(可能被重复)</p>
<p>&ndash;callback or -c:
spider method to use as callback for parsing the response
spider 中用于解析返回(response)的回调函数</p>
<p>&ndash;pipelines:
process items through pipelines
在 pipeline 中处理 item</p>
<p>&ndash;rules or -r:
use CrawlSpider rules to discover the callback (i.e. spider method) to use for parsing the response
使用 CrawlSpider 规则来发现用来解析返回(response)的回调函数</p>
<p>&ndash;noitems:
don’t show scraped items
不显示爬取到的 item</p>
<p>&ndash;nolinks:
don’t show extracted links
不显示提取到的链接</p>
<p>&ndash;nocolour:
avoid using pygments to colorize the output
避免使用 pygments 对输出着色</p>
<p>&ndash;depth or -d:
depth level for which the requests should be followed recursively (default: 1)
指定跟进链接请求的层次数(默认:1)</p>
<p>&ndash;verbose or -v:
display information for each depth level
显示每个请求的详细信息</p>
<h1 id="bench">bench</h1>
<p>这个命令会运行 benchmark 测试，模拟测试scrapy的爬取速度。</p>
<h1 id="version">version</h1>
<p>这个命令可以查询当前scrapy的版本，和一些依赖库版本信息。</p>
<p>示例：</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">➜  ~ scrapy version
Scrapy 1.3.3

➜  ~ scrapy version -v
Scrapy    : 1.3.3
lxml      : 3.7.3.0
libxml2   : 2.9.3
cssselect : 1.0.1
parsel    : 1.1.0
w3lib     : 1.17.0
Twisted   : 17.1.0
Python    : 3.5.2 <span class="o">(</span>default, Nov <span class="m">17</span> 2016, 17:05:23<span class="o">)</span> - <span class="o">[</span>GCC 5.4.0 20160609<span class="o">]</span>
pyOpenSSL : 17.0.0 <span class="o">(</span>OpenSSL 1.0.2g  <span class="m">1</span> Mar 2016<span class="o">)</span>
Platform  : Linux-4.4.0-81-generic-x86_64-with-Ubuntu-16.04-xenial
</code></pre></div></article><section class="article labels"><a class="article category" href=/categories/%E7%BC%96%E7%A8%8B/><span class="hashtag">
        <i class="fas fa-align-left"></i>
    </span>编程</a><a class="article tag" href=/tags/python/><span class="hashtag">
        <i class="fas fa-tag"></i>
    </span>Python</a><a class="article tag" href=/tags/%E7%88%AC%E8%99%AB/><span class="hashtag">
        <i class="fas fa-tag"></i>
    </span>爬虫</a><a class="article tag" href=/tags/%E6%95%99%E7%A8%8B/><span class="hashtag">
        <i class="fas fa-tag"></i>
    </span>教程</a><a class="article tag" href=/tags/scrapy/><span class="hashtag">
        <i class="fas fa-tag"></i>
    </span>Scrapy</a>
</section><section class="article navigation"><p><a class="link" href="/2017/06/25/scrapy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89/"><span class="li"></span>Scrapy学习笔记（三）</a></p><p><a class="link" href="/2017/06/20/scrapy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80/"><span class="li"></span>Scrapy学习笔记（一）</a class="link">
        </p></section><section class="article discussion"><div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "alpha87" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></section></div><section id="footer" class="footer max-body-width"><div class="footer-wrap">
  <script>
    var now = new Date();
    function createtime() {
      var grt = new Date("11/16/2016 00:00:00");
      now.setTime(now.getTime() + 250);
      days = (now - grt) / 1000 / 60 / 60 / 24;
      dnum = Math.floor(days);
      hours = (now - grt) / 1000 / 60 / 60 - 24 * dnum;
      hnum = Math.floor(hours);
      if (String(hnum).length == 1) {
        hnum = "0" + hnum;
      }
      minutes = (now - grt) / 1000 / 60 - 24 * 60 * dnum - 60 * hnum;
      mnum = Math.floor(minutes);
      if (String(mnum).length == 1) {
        mnum = "0" + mnum;
      }
      seconds =
        (now - grt) / 1000 - 24 * 60 * 60 * dnum - 60 * 60 * hnum - 60 * mnum;
      snum = Math.round(seconds);
      if (String(snum).length == 1) {
        snum = "0" + snum;
      }
      document.getElementById("timeDate").innerHTML =
        "本站已运行 " + dnum + " 天 ";
      document.getElementById("times").innerHTML =
        hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
    setInterval("createtime()", 250);
  </script>
  <p class="copyright" style="font-family: Verdana, sans-serif;">
    <i class="far fa-copyright"></i>2016-2020 Jianxun. | 岁月无婷无华✨</p>
  
  <p><span id="timeDate">正在载入天数...</span><span id="times">载入时分秒...</span></p>
  <span style="font-family: Verdana, sans-serif;">
    <a style="color: #999;" href="http://www.beian.miit.gov.cn/">京ICP备19005249号</a> |
    <a style="color: #999;" href="https://creativecommons.org/licenses/by-nc-nd/3.0/">
      <i style="color: cyan;" class="fab fa-creative-commons"></i> BY-NC-ND 3.0</a> | 
    <a href="https://lijianxun.top/post/index.xml">
      订阅 <i class="fas fa-rss"></i></a>
  </p>
</div>
</section><script src="/js/all.js" integrity=""></script></div>
</body>

</html>