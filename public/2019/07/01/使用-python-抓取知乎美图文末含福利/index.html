<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.62.2" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark">
<script data-ad-client="ca-pub-5265952723806029" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11"></script>

<script async src="https://kit.fontawesome.com/8cc90f575c.js" crossorigin="anonymous"></script><title>使用 Python 抓取知乎美图（文末含福利）&nbsp;&ndash;&nbsp;简讯</title><link rel="stylesheet" href="/css/core.min.3e4df936a2248a25a647a19d0468640da1a5f169fe50d98c35448fbf1f2d0ac37c27a6e822b983c71fba56fca9928e9d.css" integrity="sha384-Pk35NqIkiiWmR6GdBGhkDaGl8Wn&#43;UNmMNUSPvx8tCsN8J6boIrmDxx&#43;6Vvypko6d"><body>
    <div class="base-body"><section id="header" class="site header max-body-width">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><span class="site name" style="font-size: 24px;">简讯</span></a></span>
        <span class="header right-side"><div class="nav wrap">
    <nav class="nav"><a class="nav item" href="/categories/">
            <i class="fas fa-align-left"></i>&nbsp;分类
        </a><a class="nav item" href="/tags/">
            <i class="fas fa-tags"></i>&nbsp;标签
        </a><a class="nav item" href="/projects">
            
            
            <i class="fas fa-cubes"></i>&nbsp;个人项目</a><a class="nav item" href="/about">
            
            
            <i class="fas fa-feather-alt"></i>&nbsp;关于</a></nav>
</div></span></div>
        <div class="site slogan"><span class="title"></span></div></section>

<script>
    
    var typed = new Typed('.title', {
      strings: ["唯书与爱，不可辜负", "岁月无婷无华"],
      smartBackspace: true,
      typeSpeed: 95,
      backSpeed: 35
    });
</script><div id="content" class="max-body-width"><section class="article header">
    <h1 class="article title">使用 Python 抓取知乎美图（文末含福利）</h1><p class="article date">Monday, July 1, 2019<span class="reading-time"> • 4 minutes to read</span></p></section><article class="article markdown-body"><p>之前写到<strong>宅宅生活收藏夹</strong>的部署方法，见 <a href="https://lijianxun.top/2019/06/14/cjy318k78001zs26mw732ab29/"target="_blank">使用Flask，Nginx，Gunicorn，Supervisor完成网站部署</a>。这次介绍一下如何抓取知乎答案，获取知乎美图。</p>
<p>可以说爬虫是学习 Python 的入门必修课。当能独立写出第一个完整的爬虫的时候，我们已经迈出了一大步。因为在这过程中，我们已经学会了如何查看文档，学会使用 Python 相关库的操作，怎样使用 Chrome 的开发者工具（相关工具）和把抓取的数据保存到数据库中等等一系列操作，当然收获最多的还是学习 Python 的自信心。</p>
<p>下面以抓取知乎图片为例，迈出我们自学 Python 爬虫的第一步。</p>
<!-- more -->
<p>本文除 Python 相关库的使用外，还会涉及到 <code>mongo</code> 数据库的使用。</p>
<p>因为<strong>宅宅生活收藏夹</strong>只是为了收集知乎钓鱼贴的图片，有针对性，所以不能通过获取知乎首页列表全面抓取。当然抓取方式大同小异，把抓取列表改为知乎首页也是可以的。</p>
<h1 id="整体思路">整体思路</h1>
<p>首先我们需要收集知乎各类钓鱼贴，这一步可以使用爬虫（通过爬取别人已经收集的钓鱼贴，或者各种社区可能已有钓鱼贴专栏等）获取，也可以在浏览知乎时，发现合适的贴子手动添加。</p>
<p>我们需要将钓鱼贴的 ID 存入数据库中，爬虫运行时读库获取需要爬取的目标贴。</p>
<p>通过写一个爬虫，生成任务池多线程调用。将获取到的答案数据清洗，只收集答案中的图片。</p>
<p>存入库中的结构和知乎的类似：答案集合保存所有答案，每一个答案是一条独立的文档。可能写的有点绕。</p>
<p>具体结构类似这种结构：</p>
<div class="highlight"><pre class="chroma"><code class="language-javascript" data-lang="javascript"><span class="p">{</span>
    <span class="s2">&#34;Answer&#34;</span><span class="o">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&#34;id&#34;</span><span class="o">:</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s2">&#34;question_id&#34;</span><span class="o">:</span> <span class="s2">&#34;xxx&#34;</span><span class="p">,</span>
            <span class="s2">&#34;title&#34;</span><span class="o">:</span> <span class="s2">&#34;xxx&#34;</span><span class="p">,</span>
            <span class="s2">&#34;author&#34;</span><span class="o">:</span> <span class="s2">&#34;xxx&#34;</span><span class="p">,</span>
            <span class="s2">&#34;content: [
</span><span class="s2">                &#34;</span><span class="nx">imgUrl</span><span class="s2">&#34;,
</span><span class="s2">                &#34;</span><span class="p">...</span><span class="s2">&#34;
</span><span class="s2">            ]
</span><span class="s2">        },
</span><span class="s2">        {
</span><span class="s2">            &#34;</span><span class="nx">id</span><span class="s2">&#34;: 2,
</span><span class="s2">            &#34;</span><span class="nx">question_id</span><span class="s2">&#34;: &#34;</span><span class="nx">xxx</span><span class="s2">&#34;,
</span><span class="s2">            &#34;</span><span class="nx">title</span><span class="s2">&#34;: &#34;</span><span class="nx">xxx</span><span class="s2">&#34;,
</span><span class="s2">            &#34;</span><span class="nx">author</span><span class="s2">&#34;: &#34;</span><span class="nx">xxx</span><span class="s2">&#34;,
</span><span class="s2">            &#34;</span><span class="nx">content</span><span class="o">:</span> <span class="p">[</span>
                <span class="s2">&#34;imgUrl&#34;</span><span class="p">,</span>
                <span class="s2">&#34;...&#34;</span>
            <span class="p">]</span>
        <span class="p">}</span><span class="p">,</span>
        <span class="p">...</span>
    <span class="p">]</span>
<span class="p">}</span>
</code></pre></div><p>其中 <code>Answer</code> 为数据库的集合，集合中的每一项是一条回答。</p>
<p>用这种格式保存的好处是我们只需通过贴子的 ID 查询答案即可，保存起来也非常方便。</p>
<h1 id="知乎爬虫">知乎爬虫</h1>
<h2 id="开始之前">开始之前</h2>
<p>在开始之前需要安装项目依赖，只用到两个常用库：</p>
<p><code>python3 -m pip install requests pymongo</code></p>
<p>分别用来请求 URL 和操作数据库。</p>
<p>安装完成后记得启动 <code>mongo</code> 服务。</p>
<h2 id="spider">Spider</h2>
<p>爬虫代码比较简单，代码不过百行，关键找到知乎答案的接口，解析即可。而且这个接口也是非常好找的。</p>
<p>用 Chrome 的开发者工具一下就找到了&hellip;</p>
<p><a target="_blank" rel="noopener noreferrer" 
  href="https://user-images.githubusercontent.com/25655581/60421921-64c30d80-9c1d-11e9-9c45-7e4cdbe1b38a.png"><img  src="https://user-images.githubusercontent.com/25655581/60421921-64c30d80-9c1d-11e9-9c45-7e4cdbe1b38a.png"
        alt="知乎 API 截图"/></a></p>
<p>接口也没有任何加密或权限限制，在请求头中加入 <code>Cookies</code> 就可以了。是不是突然觉得很简单？</p>
<p>只要我们控制好频率，不要影响到知乎服务就行了。毕竟我们只是想获取答案而已。</p>
<p>通过 Chrome 开发者工具分析，请求携带了如下参数，我们只用到 <code>limit</code> 和 <code>offset</code>，用来控制接口返回的数量和返回的位置。</p>
<pre><code>include: data[*].is_normal,admin_closed_comment,reward_info,is_collapsed,annotation_action,annotation_detail,collapse_reason,is_sticky,collapsed_by,suggest_edit,comment_count,can_comment,content,editable_content,voteup_count,reshipment_settings,comment_permission,created_time,updated_time,review_info,relevant_info,question,excerpt,relationship.is_authorized,is_author,voting,is_thanked,is_nothelp,is_labeled,is_recognized,paid_info;data[*].mark_infos[*].url;data[*].author.follower_count,badge[*].topics
offset:
limit: 3
sort_by: default
platform: desktop
</code></pre><p>完整的请求 URL 是：</p>
<p><code>https://www.zhihu.com/api/v4/questions/21115811/answers?include=data%5B*%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_labeled%2Cis_recognized%2Cpaid_info%3Bdata%5B*%5D.mark_infos%5B*%5D.url%3Bdata%5B*%5D.author.follower_count%2Cbadge%5B*%5D.topics&amp;offset=&amp;limit=3&amp;sort_by=default&amp;platform=desktop</code></p>
<p>我们只要动态更改其中的 <code>question</code>，<code>limit</code> 和 <code>offset</code> 就可以了。</p>
<p>我们通过接口返回的答案总数，判断需要翻多少页，当然也可以通过接口返回的 <code>next</code> 和 <code>previous</code> 来获取下一页或前一页答案链接。知乎的接口设计的非常方便啊。</p>
<p>当然在翻页抓取的时候切记设置睡眠时间，放在服务器上爬的慢一点也没关系。<strong>不要影响到知乎的正常服务</strong>。</p>
<p>请求成功后我们就可以根据自己的需求，存储数据了，至于如何判断答案中是否有图片，可以参考以下代码。</p>
<p>使用到了 <code>lxml</code> 库，也可以使用 <code>re</code> 库代替。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 部分代码</span>

<span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">parse_content</span><span class="p">(</span><span class="n">content</span><span class="p">)</span><span class="p">:</span>
    <span class="sa"></span><span class="s2">&#34;&#34;&#34;</span><span class="s2">解析答案中的 content，直接获取图片</span><span class="s2">&#34;&#34;&#34;</span>

    <span class="k">if</span> <span class="sa"></span><span class="s2">&#34;</span><span class="s2">&lt;img </span><span class="s2">&#34;</span> <span class="ow">in</span> <span class="n">content</span><span class="p">:</span>
        <span class="n">img_list</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">etree</span><span class="o">.</span><span class="n">HTML</span><span class="p">(</span><span class="n">content</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">//img/@data-original</span><span class="s2">&#34;</span><span class="p">)</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">img_list</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[</span><span class="p">]</span>
</code></pre></div><p>先判断回答中是否有 <code>img</code> 标签，如果没有直接返回空列表，如果有的话，通过 <code>data-original</code> 属性获取原始大小的图片链接。也是返回一个包含图片链接的列表类型。</p>
<p>在入库的时候，我们通过 <code>parse_content</code> 的返回判断是否需要入库，如果是 <code>[]</code> 就跳过，如果列表不为空就入库。这样在之后展示的时候不会只展示作者信息，却没有回答的情况了（其实是该作者回答中没有图片）。</p>
<h2 id="调用爬虫">调用爬虫</h2>
<p>当我们完成上述操作，就可以单独写一个文件使用多线程调用爬虫了。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">concurrent.futures</span> <span class="kn">import</span> <span class="n">ThreadPoolExecutor</span>


<span class="n">qid_list</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">get_all_question</span><span class="p">(</span><span class="p">)</span>
<span class="n">crawler_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">ZhihuSpider</span><span class="p">(</span><span class="n">qid</span><span class="p">)</span><span class="o">.</span><span class="n">run</span> <span class="k">for</span> <span class="n">qid</span> <span class="ow">in</span> <span class="n">qid_list</span><span class="p">]</span>

<span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
    <span class="n">future</span> <span class="o">=</span> <span class="p">[</span><span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">task</span><span class="p">)</span> <span class="k">for</span> <span class="n">task</span> <span class="ow">in</span> <span class="n">crawler_list</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">future</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">result</span><span class="p">(</span><span class="p">)</span>
</code></pre></div><p><code>qid_list</code> 来自查库获取所有的知乎贴子 ID。</p>
<p>使用 <code>concurrent.futures</code> 库并行执行任务，在我们的代码里使用 <code>ThreadPoolExecutor</code>，它使用线程池来异步执行调用。</p>
<p><code>max_workers</code> 控制最多线程的使用，本例中使用最多4个线程执行任务。</p>
<p>具体文档见 <a href="https://docs.python.org/zh-cn/3/library/concurrent.futures.html#threadpoolexecutor"target="_blank">ThreadPoolExecutor</a>，是对 <code>thread</code> 库的封装，让我们使用线程时更加简单。</p>
<p>完整代码见 <a href="https://github.com/alpha87/zhihuFish"target="_blank">github</a>。</p>
<p>最后总结一下整体思路：我们先把需要抓取的 ID 存入数据库，作为任务池，爬虫调取任务池中的 ID，将爬取结果再保存到数据库。</p>
<p>等我们学会了 Python 爬虫，再学 Web 开发的时候，还能将我们抓取的结果展示出来。一不小心又学习了 Python Web 开发，多好。</p>
<h1 id="福利">福利</h1>
<p>当我们爬虫写好，入库，并成功展示出来，不知不觉就实现了一个小程序项目：<strong>宅宅生活收藏夹</strong>。</p>
<p><a target="_blank" rel="noopener noreferrer" 
  href="https://user-images.githubusercontent.com/25655581/60482994-3c431e00-9cc6-11e9-9344-3c50b9dbef10.png"><img  src="https://user-images.githubusercontent.com/25655581/60482994-3c431e00-9cc6-11e9-9344-3c50b9dbef10.png"
        alt="小程序首页展示"/></a></p>
</article><section class="article labels"><a class="article category" href=/categories/python/><span class="hashtag">
        <i class="fas fa-align-left"></i>
    </span>Python</a><a class="article tag" href=/tags/%E7%88%AC%E8%99%AB/><span class="hashtag">
        <i class="fas fa-tag"></i>
    </span>爬虫</a><a class="article tag" href=/tags/%E7%9F%A5%E4%B9%8E/><span class="hashtag">
        <i class="fas fa-tag"></i>
    </span>知乎</a><a class="article tag" href=/tags/%E7%BE%8E%E5%9B%BE/><span class="hashtag">
        <i class="fas fa-tag"></i>
    </span>美图</a>
</section><section class="article navigation"><p><a class="link" href="/2019/07/03/%E6%96%B0%E4%B9%B0%E6%9D%A5%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A6%81%E6%80%8E%E4%B9%88%E9%85%8D%E7%BD%AE/"><span class="li"></span>新买来的服务器要怎么配置</a></p><p><a class="link" href="/2019/06/20/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E8%BE%93%E5%85%A5%E6%A1%86%E7%9A%84%E4%BD%BF%E7%94%A8/"><span class="li"></span>微信小程序输入框的使用</a class="link">
        </p></section><section class="article discussion"><div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "alpha87" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></section></div><section id="footer" class="footer max-body-width"><div class="footer-wrap">
  <script>
    var now = new Date();
    function createtime() {
      var grt = new Date("11/16/2016 00:00:00");
      now.setTime(now.getTime() + 250);
      days = (now - grt) / 1000 / 60 / 60 / 24;
      dnum = Math.floor(days);
      hours = (now - grt) / 1000 / 60 / 60 - 24 * dnum;
      hnum = Math.floor(hours);
      if (String(hnum).length == 1) {
        hnum = "0" + hnum;
      }
      minutes = (now - grt) / 1000 / 60 - 24 * 60 * dnum - 60 * hnum;
      mnum = Math.floor(minutes);
      if (String(mnum).length == 1) {
        mnum = "0" + mnum;
      }
      seconds =
        (now - grt) / 1000 - 24 * 60 * 60 * dnum - 60 * 60 * hnum - 60 * mnum;
      snum = Math.round(seconds);
      if (String(snum).length == 1) {
        snum = "0" + snum;
      }
      document.getElementById("timeDate").innerHTML =
        "本站已运行 " + dnum + " 天 ";
      document.getElementById("times").innerHTML =
        hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
    setInterval("createtime()", 250);
  </script>
  <p class="copyright" style="font-family: Verdana, sans-serif;">
    <i class="far fa-copyright"></i>2016-2020 Jianxun. | 岁月无婷无华✨</p>
  
  <p><span id="timeDate">正在载入天数...</span><span id="times">载入时分秒...</span></p>
  <span style="font-family: Verdana, sans-serif;">
    <a style="color: #999;" href="http://www.beian.miit.gov.cn/">京ICP备19005249号</a> |
    <a style="color: #999;" href="https://creativecommons.org/licenses/by-nc-nd/3.0/">
      <i style="color: cyan;" class="fab fa-creative-commons"></i> BY-NC-ND 3.0</a>
  </p>
</div>
</section><script src="/js/hljs.min.0799348a91dce12c6be4a73f943cfe78f181f4e6f6ec35c4af0fca1de377879f77cfab03c30f03a174d675737b5a9314.js" integrity="sha384-B5k0ipHc4Sxr5Kc/lDz&#43;ePGB9Ob27DXErw/KHeN3h593z6sDww8DoXTWdXN7WpMU"></script><script>hljs.initHighlightingOnLoad();</script></div>
</body>

</html>